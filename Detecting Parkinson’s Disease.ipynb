#!/usr/bin/env python
# coding: utf-8

#MAKE NECESSARY IMPORT 

import numpy as np
import pandas as pd
import os, sys
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#READING THE DATA-SET


df = pd.read_csv('parkinsons.data')
df.head()

#Get the features and labels from the DataFrame (dataset). 
#The features are all the columns except ‘status’, and the labels are those in the ‘status’ column."""

features=df.loc[:,df.columns!='status'].values[:,1:]
labels=df.loc[:,'status'].values

#The ‘status’ column has values 0 and 1 as labels; let’s get the counts of these labels for both- 0 and 1.

print(labels[labels==1].shape[0], labels[labels==0].shape[0])


#Initialize an XGBClassifier and train the model. 
#This classifies using eXtreme Gradient Boosting- using gradient boosting algorithms for modern data science problems. 
#It falls under the category of Ensemble Learning in ML, where we train and predict using many models to produce one superior output. """

model=XGBClassifier()
model.fit(x_train,y_train)

#Finally, generate y_pred (predicted values for x_test) and calculate the accuracy for the model. Print it out.

y_pred=model.predict(x_test)
print(accuracy_score(y_test, y_pred)*100)


#ACCURACY OBTAINED : 94.87179487179486 

